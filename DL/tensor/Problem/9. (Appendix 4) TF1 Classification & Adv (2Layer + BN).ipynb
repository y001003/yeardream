{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "<br>이제 higher level API(tf.layers 등)를 적극 활용하면서 필요에 따라 앞서 배운 low level API(tf.nn)를 활용해 세부적인 model tuning이 가능합니다. (https://goo.gl/Rmy8qq)\n",
    "<br>\n",
    "<br><span style=\"color:red;\"> - 더욱 편하게 layer 를 구성할 수 있도록 돕는 **tf.layers** 를 적용합니다.\n",
    "<br>- tf.layers.batch_normalization()을 활용해 손쉽게 **Batch Normalization**을 적용할 수 있습니다.</span>\n",
    "<br>- BN을 적용하면 전반적으로 모델의 성능이 향상되어 Params init, Regularization, Dropout 등의 필요성이 크게 줄어듭니다. \n",
    "<br>- 물론 신경망이 깊어지고 풀어야 할 문제가 복잡해진다면 앞선 최적화 방법들을 함께 적용시켜 성능 향상을 도모할 수 있습니다. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import os, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, utils\n",
    "\n",
    "(train_data, train_label), (test_data, test_label) = datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data.reshape(60000, 784) / 255.0\n",
    "test_data = test_data.reshape(10000, 784) / 255.0\n",
    "\n",
    "train_label = utils.to_categorical(train_label) # 0~9 -> one-hot vector\n",
    "test_label = utils.to_categorical(test_label) # 0~9 -> one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각종 placeholder 들을 선언해줍니다.\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "bn_sign = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BN 순서 : 선형 결합 -> BN 적용 -> 활성화 함수 \n",
    "# activation function 을 걷어내고 BN을 먼저 적용하기 위해 activation에 None을 적용하였습니다.\n",
    "\n",
    "L1 = tf.layers.dense(X, 256, activation=None) \n",
    "L1 = tf.layers.batch_normalization(L1, training=bn_sign)\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "L2 = tf.layers.dense(L1, 256, activation=None)\n",
    "L2 = tf.layers.batch_normalization(L2, training=bn_sign)\n",
    "L2 = tf.nn.relu(L2)\n",
    "\n",
    "model = tf.layers.dense(L2, 10, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.losses.softmax_cross_entropy(Y, model) \n",
    "optimizer = tf.train.AdamOptimizer(1e-3).minimize(cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.losses.softmax_cross_entropy(Y, model) \n",
    "\n",
    "# BN 적용 중 계산되는 moving_mean & moving_variance 를 지속적으로 업데이트 해줍니다.\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3).minimize(cost) \n",
    "    \n",
    "# * When is_training is \"True\", the moving_mean and moving_variance need to be updated, \n",
    "# by default the update_ops are placed in tf.GraphKeys.UPDATE_OPS\n",
    "# so they need to be added as a dependency to the train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "total_batch = int(len(train_data) / batch_size)\n",
    "total_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 || Avg. Training cost = 0.166 || Training accuracy : 0.939\n",
      "Epoch: 2 || Avg. Training cost = 0.057 || Training accuracy : 0.981\n",
      "Epoch: 3 || Avg. Training cost = 0.029 || Training accuracy : 0.988\n",
      "Epoch: 4 || Avg. Training cost = 0.015 || Training accuracy : 0.991\n",
      "Epoch: 5 || Avg. Training cost = 0.009 || Training accuracy : 0.993\n",
      "Epoch: 6 || Avg. Training cost = 0.007 || Training accuracy : 0.993\n",
      "Epoch: 7 || Avg. Training cost = 0.006 || Training accuracy : 0.994\n",
      "Epoch: 8 || Avg. Training cost = 0.005 || Training accuracy : 0.995\n",
      "Epoch: 9 || Avg. Training cost = 0.004 || Training accuracy : 0.996\n",
      "Epoch: 10 || Avg. Training cost = 0.003 || Training accuracy : 0.997\n",
      "Epoch: 11 || Avg. Training cost = 0.003 || Training accuracy : 0.996\n",
      "Epoch: 12 || Avg. Training cost = 0.003 || Training accuracy : 0.996\n",
      "Epoch: 13 || Avg. Training cost = 0.002 || Training accuracy : 0.997\n",
      "Epoch: 14 || Avg. Training cost = 0.002 || Training accuracy : 0.997\n",
      "Epoch: 15 || Avg. Training cost = 0.002 || Training accuracy : 0.997\n",
      "Learning process is completed!\n"
     ]
    }
   ],
   "source": [
    "# import tqdm \n",
    "# for epoch in tqdm.notebook.tqdm(range(15)):\n",
    "\n",
    "for epoch in range(15):\n",
    "    \n",
    "    \n",
    "    training_results = [] # Training accuracy 를 동시에 출력해보도록 합니다.\n",
    "    total_cost = 0\n",
    "    batch_idx = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        \n",
    "        batch_x = train_data[ batch_idx : batch_idx + batch_size ]\n",
    "        batch_y = train_label[ batch_idx : batch_idx + batch_size ]\n",
    "        \n",
    "        # 1) Optimizer\n",
    "        sess.run(optimizer, feed_dict={X: batch_x, \n",
    "                                       Y: batch_y, \n",
    "                                       bn_sign: True}) # Batch Normalization - Training mode\n",
    "        \n",
    "        # 2) Cost\n",
    "        batch_cost = sess.run(cost, feed_dict={X: batch_x, \n",
    "                                               Y: batch_y, \n",
    "                                               bn_sign: True}) # Batch Normalization - Training mode\n",
    "        total_cost = total_cost + batch_cost\n",
    "        \n",
    "        \n",
    "        # 3) 매 Epoch마다 Training accuracy를 출력합니다. (bn_sign을 False로 바꾸어 training mode가 아닌 inference mode로 실행합니다.)\n",
    "        batch_results = sess.run([is_correct], feed_dict={X: batch_x, \n",
    "                                                           Y: batch_y, \n",
    "                                                           bn_sign: False}) \n",
    "        training_results = training_results + batch_results\n",
    "        \n",
    "        batch_idx += batch_size\n",
    "    \n",
    "    \n",
    "    training_cost = total_cost / total_batch\n",
    "    \n",
    "    \n",
    "    print('Epoch: {}'.format(epoch + 1), \n",
    "          '|| Avg. Training cost = {:.3f}'.format(training_cost),\n",
    "          '|| Training accuracy : {:.3f}'.format(np.mean(training_results)))\n",
    "\n",
    "print('Learning process is completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.9783999919891357\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy 를 출력합니다. \n",
    "# bn_sign을 False로 바꾸어 training mode가 아닌 [ inference mode ]로 바꿔주어야 합니다.\n",
    "# 학습 단계에서는 데이터가 배치 단위로 들어오기 때문에 배치의 평균, 분산을 구하는 것이 가능하지만, \n",
    "# 테스트 단계에서는 배치 단위로 평균/분산을 구하기가 어려워 학습 단계에서 배치 단위의 평균/분산을 저장해 놓고 테스트 시에는 이를 사용합니다.\n",
    "\n",
    "print('Test accuracy : {}'.format(sess.run(accuracy, \n",
    "                                           feed_dict={\n",
    "                                               X: test_data,\n",
    "                                               Y: test_label,\n",
    "                                               bn_sign: False})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
