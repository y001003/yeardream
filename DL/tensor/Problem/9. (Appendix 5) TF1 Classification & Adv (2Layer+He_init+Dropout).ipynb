{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "<br>이제 higher level API(tf.layers 등)를 적극 활용하면서 필요에 따라 앞서 배운 low level API(tf.nn)를 활용해 세부적인 model tuning이 가능합니다. (https://goo.gl/Rmy8qq)\n",
    "<br>\n",
    "<br><span style=\"color:red;\"> - 더욱 편하게 layer 를 구성할 수 있도록 돕는 **tf.layers** 를 적용합니다.\n",
    "<br>- 모델 Parameter 초기화 방법 중 하나인 **He initialization**을 적용합니다.\n",
    "<br>- layers.dropout()을 통해 **Dropout**을 layer마다 다른 비율로 적용할 수 있습니다.</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import os, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, utils\n",
    "\n",
    "(train_data, train_label), (test_data, test_label) = datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data.reshape(60000, 784) / 255.0\n",
    "test_data = test_data.reshape(10000, 784) / 255.0\n",
    "\n",
    "train_label = utils.to_categorical(train_label) # 0~9 -> one-hot vector\n",
    "test_label = utils.to_categorical(test_label) # 0~9 -> one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각종 placeholder 들을 선언해줍니다.\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "dropout_sign = tf.placeholder(tf.bool) # layers.dropout() 은 True/False 로 Training/Testing 여부를 결정해 줄 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서 어렵게 진행했던 layer architecture 구성을 다음과 같이 편하게 할 수 있습니다.\n",
    "\n",
    "L1 = tf.layers.dense(X, 256, activation=tf.nn.relu) # Hidden layer 1\n",
    "L2 = tf.layers.dense(L1, 256, activation=tf.nn.relu) # Hidden layer 2\n",
    "model = tf.layers.dense(L2, 10, activation=None) # 10 == # of label's columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 코드를 아래와 같이 변경하면 he 초기화 방법과 dropout을 추가로 적용할 수 있습니다.\n",
    "\n",
    "L1 = tf.layers.dense(X, 256, activation=tf.nn.relu, kernel_initializer=tf.keras.initializers.he_normal()) # he (keras)\n",
    "L1 = tf.layers.dropout(L1, rate=0.2, training=dropout_sign) # layers.dropout()의 \"rate\"는 keeping rate가 아닌 dropping rate이며, default 값은 0.5입니다.\n",
    "L2 = tf.layers.dense(L1, 256, activation=tf.nn.relu, kernel_initializer=tf.keras.initializers.he_normal()) # he (keras)\n",
    "L2 = tf.layers.dropout(L2, rate=0.2, training=dropout_sign) \n",
    "model = tf.layers.dense(L2, 10, activation=None) # 10 == # of label's columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.layers.dense(inputs, outputs(은닉층의 노드 수), activation_fn, kernel_init(가중치 초기화 방법), use_bias=True(default))\n",
    "# - output result 는 [ activation(inputs * kernel + bias) ] 의 방식으로 계산됩니다.\n",
    "# - kernel_initializer(가중치 초기화 방법)의 default 값 : [ glorot uniform initializer (= Xavier init) ], 설명 @ https://goo.gl/2Av59i\n",
    "# - bias_initializer(bias 초기화 방법)의 default 값 : [ tf.zeros_initializer() (= 0) ]\n",
    "\n",
    "L1 = tf.layers.dense(X, 256, activation=tf.nn.relu, kernel_initializer=tf.keras.initializers.he_normal()) # he (keras, he_uniform() is also available)\n",
    "# L1 = tf.layers.dense(X, 256, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer()) # he (contrib)\n",
    "# L1 = tf.layers.dense(X, 256, activation=tf.nn.relu, kernel_initializer=tf.keras.initializers.glorot_normal()) # xavier (keras, glorot_uniform() is also available)\n",
    "# L1 = tf.layers.dense(X, 256, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer()) # xavier (contrib)\n",
    "L1 = tf.layers.dropout(L1, rate=0.2, training=dropout_sign)\n",
    "\n",
    "L2 = tf.layers.dense(L1, 256, activation=tf.nn.relu, kernel_initializer=tf.keras.initializers.he_normal()) # he (keras, he_uniform() is also available)\n",
    "# L2 = tf.layers.dense(L1, 256, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer()) # he (contrib)\n",
    "# L2 = tf.layers.dense(L1, 256, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer()) # xavier (contrib)\n",
    "# L2 = tf.layers.dense(L1, 256, activation=tf.nn.relu, kernel_initializer=tf.keras.initializers.glorot_normal()) # xavier (keras, glorot_uniform() is also available)\n",
    "L2 = tf.layers.dropout(L2, rate=0.2, training=dropout_sign)\n",
    "\n",
    "model = tf.layers.dense(L2, 10, activation=None) \n",
    "\n",
    "\n",
    "# (extra) 초기화 함수 간 관계는 아래와 같습니다. (동일한 것 중 어느 것이든 택하여 쓰셔도 됩니다) @ https://goo.gl/XZESC6\n",
    "# tf.contrib.layers.variance_scaling_initializer(uniform=False) == tf.keras.initializers.he_normal()\n",
    "# tf.contrib.layers.variance_scaling_initializer(uniform=True) == tf.keras.initializers.he_uniform()\n",
    "# tf.contrib.layers.xavier_initializer(uniform=False) == tf.keras.initializers.glorot_normal()\n",
    "# tf.contrib.layers.xavier_initializer(uniform=True) == tf.keras.initializers.glorot_uniform()\n",
    "# * 가능한 가중치 초기화 방식 (keras) @ https://goo.gl/zia5uK \n",
    "\n",
    "# (extra) 추가로 원할 경우 layers.dense()에 L1 이나 L2 정규화를 적용할 수 있습니다.\n",
    "# ,kernel_regularizer=tf.contrib.layers.l1_regularizer()\n",
    "# ,kernel_regularizer=tf.contrib.layers.l2_regularizer()\n",
    "\n",
    "# (extra) tf.layers.dense() 대신 tf.contrib.layers.fully_connencted()를 활용할 수도 있습니다.\n",
    "# -> L1 = tf.contrib.layers.fully_connected(X, 256, activation_fn=tf.nn.relu, weights_initializer=tf.keras.initializers.he_normal())\n",
    "# * 2가지 방식 모두 근본적으로 동일합니다 (fully_connected()가 사실 dense()를 호출합니다. fully_connected()는 dense()에 몇 가지 추가적인 기능을 더한 함수입니다. fully_connected()는 기본 활성화함수가 relu이며 dense()는 linear입니다.) @ https://goo.gl/ayVudM\n",
    "# * contrib module containing volatile or experimental code. (Ops for building neural network layers, regularizers, summaries, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.losses 에는 보다 편하게 cost function 들을 구성할 수 있는 함수들이 구현되어 있습니다.\n",
    "\n",
    "cost = tf.losses.softmax_cross_entropy(Y, model) \n",
    "optimizer = tf.train.AdamOptimizer(1e-3).minimize(cost) # 1e-3 == 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "total_batch = int(len(train_data) / batch_size)\n",
    "total_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 || Avg. Training cost = 0.291 || Training accuracy : 0.930\n",
      "Epoch: 2 || Avg. Training cost = 0.123 || Training accuracy : 0.971\n",
      "Epoch: 3 || Avg. Training cost = 0.086 || Training accuracy : 0.981\n",
      "Epoch: 4 || Avg. Training cost = 0.066 || Training accuracy : 0.986\n",
      "Epoch: 5 || Avg. Training cost = 0.055 || Training accuracy : 0.990\n",
      "Epoch: 6 || Avg. Training cost = 0.046 || Training accuracy : 0.992\n",
      "Epoch: 7 || Avg. Training cost = 0.040 || Training accuracy : 0.994\n",
      "Epoch: 8 || Avg. Training cost = 0.035 || Training accuracy : 0.995\n",
      "Epoch: 9 || Avg. Training cost = 0.030 || Training accuracy : 0.996\n",
      "Epoch: 10 || Avg. Training cost = 0.027 || Training accuracy : 0.997\n",
      "Epoch: 11 || Avg. Training cost = 0.025 || Training accuracy : 0.997\n",
      "Epoch: 12 || Avg. Training cost = 0.024 || Training accuracy : 0.997\n",
      "Epoch: 13 || Avg. Training cost = 0.023 || Training accuracy : 0.998\n",
      "Epoch: 14 || Avg. Training cost = 0.022 || Training accuracy : 0.998\n",
      "Epoch: 15 || Avg. Training cost = 0.021 || Training accuracy : 0.998\n",
      "Learning process is completed!\n"
     ]
    }
   ],
   "source": [
    "# import tqdm \n",
    "# for epoch in tqdm.notebook.tqdm(range(15)):\n",
    "\n",
    "for epoch in range(15):\n",
    "    \n",
    "    \n",
    "    training_results = [] # Training accuracy 를 동시에 출력해보도록 합니다.\n",
    "    total_cost = 0\n",
    "    batch_idx = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        \n",
    "        batch_x = train_data[ batch_idx : batch_idx + batch_size ]\n",
    "        batch_y = train_label[ batch_idx : batch_idx + batch_size ]\n",
    "        \n",
    "        # 1) Optimizer\n",
    "        sess.run(optimizer, feed_dict={X: batch_x, \n",
    "                                       Y: batch_y, \n",
    "                                       dropout_sign: True}) # Dropout - ON\n",
    "        \n",
    "        # 2) Cost\n",
    "        batch_cost = sess.run(cost, feed_dict={X: batch_x, \n",
    "                                               Y: batch_y, \n",
    "                                               dropout_sign: True}) # Dropout - ON\n",
    "        total_cost = total_cost + batch_cost\n",
    "        \n",
    "        \n",
    "        # 3) 매 Epoch마다 Training accuracy를 출력합니다. (dropout_sign을 False로 바꾸어 dropout을 걷어내줘야 합니다.)\n",
    "        batch_results = sess.run([is_correct], feed_dict={X: batch_x, \n",
    "                                                           Y: batch_y, \n",
    "                                                           dropout_sign: False}) \n",
    "        training_results = training_results + batch_results\n",
    "        \n",
    "        batch_idx += batch_size\n",
    "    \n",
    "    \n",
    "    training_cost = total_cost / total_batch\n",
    "    \n",
    "    \n",
    "    print('Epoch: {}'.format(epoch + 1), \n",
    "          '|| Avg. Training cost = {:.3f}'.format(training_cost),\n",
    "          '|| Training accuracy : {:.3f}'.format(np.mean(training_results)))\n",
    "\n",
    "print('Learning process is completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.9812999963760376\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy 를 출력합니다. dropout_sign을 False로 바꾸어 dropout을 걷어내줘야 합니다.\n",
    "\n",
    "print('Test accuracy : {}'.format(sess.run(accuracy, \n",
    "                                           feed_dict={\n",
    "                                               X: test_data,\n",
    "                                               Y: test_label,\n",
    "                                               dropout_sign: False})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
